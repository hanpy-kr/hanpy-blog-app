---
deployment: false
category: Frontend
title: 'next-sitemap'
summary: 'sitemap에 대해 알아보고, nextjs에 sitemap을 적용시켜 봅시다.'
pageKey: frontend-nextjs-sitemap
lng: KOR
publishedAt: 2022-03-01
---

# Nextjs Sitemap

## Sitemap이란

Sitemap이란, 검색 사이트(google)의 크롤링 봇들이 우리 Application을 사용할 수 있도록 정보를 주기위해 활용됩니다. 따라서 서비스를 배포한다면, 관련 설정들을 추가하여 만든 Application에 많이 접속할 수 있도록 만들 필요가 있습니다.

## robots.txt

Sitemap을 통해 Application이 검색 될 수 있도록 등록을 하였다면, 만든 Application에서 크롤링을 하면 안되는 부분에 대해 명시를 하는 것도 필요합니다. 이러한 명시를 적어주는 곳이 roboots.txt 입니다.

Sitemap과 Robots.txt는 검색엔진최적화(SEO) 설정에서 기초적인 부분이라 할 수 있다. 이러한 기본설정을 next-sitemap을 통해 간단히 설정 가능하다.

## Nextjs Sitemap 시작하기

### 1. 설치

```bash
$ npm install -D next-sitemap
```

### 2. postbuild 추가

```json
// package.json
{
  "scripts": {
    "postbuild": "next-sitemap"
  }
}
```

### 3. next-sitemap.config.js 생성

root경로에 next-sitemap.config.js 파일을 하나 생성해 줍니다.

```javascript
/** @type {import('next-sitemap').IConfig} */

module.exports = {
  siteUrl: 'http://localhost:8088', // .게시하는 site의 url
  generateRobotsTxt: true, // robots.txt generate 여부 (자동생성 여부)
  sitemapSize: 7000, // sitemap별 최대 크기 (최대 크기가 넘어갈 경우 복수개의 sitemap으로 분리됨)
  changefreq: 'daily', // 페이지 주소 변경 빈도 (검색엔진에 제공됨) - always, daily, hourly, monthly, never, weekly, yearly 중 택 1
  priority: 1, // 페이지 주소 우선순위 (검색엔진에 제공됨, 우선순위가 높은 순서대로 크롤링함)
  exclude: [
    '/exclude/review', // 페이지 주소 하나만 제외시키는 경우
    '/exclude/**', // 하위 주소 전체를 제외시키는 경우
  ], // sitemap 등록 제외 페이지 주소
  robotsTxtOptions: {
    // 정책 설정
    policies: [
      {
        userAgent: '*', // 모든 agent 허용
        allow: '/', // 모든 페이지 주소 크롤링 허용
        disallow: [
          '/exclude', // exclude로 시작하는 페이지 주소 크롤링 금지
        ],
      },
      // 추가 정책이 필요할 경우 배열 요소로 추가 작성
    ],
  }, // robots.txt 옵션 설정
}
```

### 4. build

```
$ npm run build
```

---

## Reference

배포 후에 다시 정리를 하자

- https://jforj.tistory.com/311
